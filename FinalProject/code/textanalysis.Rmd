---
title: "Text analysis"
author: "YZ Analytics"
date: ""
output: 
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---

```{r, setup, include=FALSE}
library(tidyverse)
library(tm)
library(text2vec)
library(tidytext)
library(textmineR)
library(SnowballC)
library(caret)
#library(lightgbm)
```

```{r}
Wine <- read_csv("../data/winemag-data-130k-v2.csv",
                 col_types = cols(
                     X1 = col_double(),
                     country = col_character(),
                     description = col_character(),
                     designation = col_character(),
                     points = col_double(),
                     price = col_double(),
                     province = col_character(),
                     region_1 = col_character(),
                     region_2 = col_character(),
                     taster_name = col_character(),
                     taster_twitter_handle = col_character(),
                     title = col_character(),
                     variety = col_character(),
                     winery = col_character()),
                 progress = FALSE
                 ) %>%
    rename(id = X1)
```


### Calculating Term Frequency-Inverse Document Frequency

Code based from https://www.tidytextmining.com/tfidf.html.

Term Frequency-Inverse Document Frequency (TF-IDF) is a statistic that evaluates how important a word is in a document or corpus. It is calculated through dividing the term frequency of how often a word appears in a document by its inverse document frequency, which is the inverse of the proportion of how many documents in a corpus have a certain term.  Therefore, high frequency terms but with little importance such as "the" or "and" will have low TF-IDF values, and so TF-IDF can be used as a weighting measure in ranking.

To use TF-IDF in our prediction model, we will use a sum of the TF-IDF per description as a variable.

```{r}
Wine_tfidf <- Wine %>%
  unnest_tokens(word, description) %>%
  count(points, id, word, sort = TRUE) %>%
  bind_tf_idf(word, id, n) %>%
  group_by(id) %>% 
  summarise(tf_idf = sum(tf_idf))

Wine2 <- left_join(Wine, Wine_tfidf, by = "id")
```

First let's look at TF-IDF by points:

```{r}
Wine_points_tfidf <- Wine %>%
  unnest_tokens(word, description) %>%
  count(points, word, sort = TRUE) %>%
  bind_tf_idf(word, points, n)
```

```{r}
Wine_points_tfidf %>%
  filter(points == 100) %>%
  arrange(desc(tf_idf)) %>%
  head()
```

We see that the words with the highest TF-IDF values are the unique words in the 100-point wine descriptions that occur only 1-2 in the vocabulary of all the descriptions. 

Let's look specifically at the words with the highest TF-IDF values for 80-point wines:

```{r}
Wine_points_tfidf %>%
  filter(points == 80) %>%
  arrange(desc(tf_idf)) %>%
  head()
```

These words occur more frequently than the words in the 100-point descriptions. However, the frequencies are pretty low. It might be useful to separate points into different levels (perhaps 80-86 is low rating, 97-93 is medium, and 94-100 is high).

```{r}
Wine$rating <- cut(Wine$points,
                   breaks=c(-Inf, 86, 93, Inf),
                   labels=c("low","medium","high"))
```

```{r}
Wine_rating_tfidf <- Wine %>%
  unnest_tokens(word, description) %>%
  count(rating, word, sort = TRUE) %>%
  bind_tf_idf(word, rating, n)
```

```{r}
Wine_rating_tfidf %>%
  filter(rating == "high") %>%
  arrange(desc(tf_idf)) %>%
  head()
```

We can also look at TF-IDF based on variables other than points. For example, we can look at TF-IDF values based on variety of wine.

```{r}
wine_words <- Wine %>%
  unnest_tokens(word, description) %>%
  count(variety, word, sort = TRUE)
```

```{r}
plot_wine <- wine_words %>%
  bind_tf_idf(word, variety, n) %>%
  mutate(word = fct_reorder(word, tf_idf)) %>%
  mutate(variety = factor(variety, levels = c("Pinot Noir",
                                            "Cabernet Sauvignon", 
                                            "Chardonnay",
                                            "Red Blend", 
                                            "Riesling")))

plot_wine %>% 
  group_by(variety) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = variety)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~variety, ncol = 2, scales = "free") +
  coord_flip()
```

A lot of the words with high TF-IDF values by variety are obvious - for example, the word with the highest TF-IDF value for riesling is "riesling."

Following https://www.kaggle.com/nnnnick/predicting-wine-ratings-using-lightgbm-text2vec, we can look at words with the highest and lowest mean scores:

```{r}
wine_explore <- Wine %>% 
    select(description, points) %>%
    mutate(description = gsub('[[:punct:] ]+',' ',tolower(description)))

words <- str_split(wine_explore$description, ' ')
all_words <- data.frame(points = rep(wine_explore$points, sapply(words, length)), words = unlist(words))
```

```{r}
words_grouped <- all_words %>%
    group_by(words) %>%
    summarize(
        points = mean(points),
        count = n()
    ) %>%
    filter(count > 10) %>%
    arrange(desc(points))

top <- words_grouped[1:10,] %>% cbind(top_bottom = 'top')
bottom <- words_grouped[(nrow(words_grouped) - 9):nrow(words_grouped),] %>% cbind(top_bottom = 'bottom')
top_bottom <- rbind(top, bottom)

ggplot(top_bottom, aes(x = reorder(words, points), y = points, fill = top_bottom)) +
    geom_bar(stat = 'identity') +
    coord_flip() + 
    scale_fill_manual(values = c('#00b4fb', '#fa6560')) + 
    ggtitle('Wine Review Words with the Highest and Lowest Mean Points', subtitle = NULL) +
    xlab('Average Points') +
    ylab('Word') + 
    labs(fill = 'Top or Bottom')
```

Nobody wants to drink a wine that's described as "gluey" or "fake." How often do these words show up though? To find out, we need to create a document-term matrix, which shows the the frequency of terms that occur in a collection of documents.

### Creating a Document-Term Matrix

DTM created from code here: https://datawarrior.wordpress.com/2018/01/22/document-term-matrix-text-mining-in-r-and-python/

```{r}
#Create DTM
dtm <- CreateDtm(Wine$description,
                doc_names = Wine$id,
                ngram_window = c(1, 1),
                lower = TRUE,
                remove_punctuation = TRUE,
                remove_numbers = TRUE,
                stem_lemma_function = wordStem)
```

The document-term matrix that is created is huge - about 43 megabytes with close to 3 billion elements. We need to create some functions that will allow us to use the DTM:

```{r}
#Create functions
get.token.occurrences<- function(dtm, token)
  dtm[, token] %>% as.data.frame() %>% rename(count=".") %>%
  mutate(token=row.names(.)) %>% arrange(-count)
 
get.total.freq<- function(dtm, token) dtm[, token] %>% sum
 
get.doc.freq<- function(dtm, token)
  dtm[, token] %>% as.data.frame() %>% rename(count=".") %>%
  filter(count>0) %>% pull(count) %>% length
```

Now we can see how many wines are actually described as "fake":

```{r}
dtm %>% get.doc.freq(wordStem("fake"))
```

Which 13 wines?

```{r}
fakewines <- dtm %>% get.token.occurrences(wordStem("fake")) %>% head(13)
Wine$title[c(as.numeric(fakewines$token))]
```

Let's look at the description of the fourth wine, Love 2015 Cabernet Sauvignon (Vino de la Tierra de Castilla):

```{r}
Wine$description[27585] #27585 is the token number for the fourth wine
```

Yikes. This seems like a bad wine.


### Prediction

Let's look at predicting wines.

Because we have a lot of missing data and a mixture of numerical and categorical data, methods like random forest are difficult to implement.

could look at k-prototypes clustering? https://cran.r-project.org/web/packages/clustMixType/clustMixType.pdf or k-modes clustering?
could also look at gradient boosting like in the example above

**Light gradient boosting**

First, we'll adjust our dataset and remove the id, description, title, and taster_twitter_handle variables.

```{r}
Wine3 <- Wine2 %>%
  select(-id, -description, -title, -region_2, -taster_twitter_handle) %>%
  mutate(country = as.factor(country),
         designation = as.factor(designation),
         province = as.factor(province),
         region_1 = as.factor(region_1),
         taster_name = as.factor(taster_name),
         variety = as.factor(variety),
         winery = as.factor(winery))
```


Next, we will split our data into training and test sets.

```{r}
set.seed(1)
smp_size <- floor(0.8 * nrow(Wine2))
train_ind <- sample(seq_len(nrow(Wine2)), size = smp_size)
train <- Wine2[train_ind, ]
test <- Wine2[-train_ind, ]
```

Next, we will use a function that will clean the data.

```{r}
clean <- function(train, test) {
  prep_fun <- tolower
  tok_fun <- function(x) {
    word_tokenizer(x) %>% lapply(function(x) SnowballC::wordStem(x, language='en'))
  }
  
  it_train <- itoken(train$description, 
                    preprocessor = prep_fun, 
                    tokenizer = tok_fun, 
                    ids = train$X1, 
                    progressbar = T)
  vocab <- create_vocabulary(it_train, stopwords = stopwords('en'), ngram = c(1L, 2L))
  
  pruned_vocab <- prune_vocabulary(vocab, 
                                  term_count_min = 10, 
                                  doc_proportion_max = 0.5,
                                  doc_proportion_min = 0.001)
  # Create Document-Term Matrix
  vectorizer <- vocab_vectorizer(pruned_vocab)
  dtm_train <- create_dtm(it_train, vectorizer)

  # define tfidf model
  tfidf <- TfIdf$new()
  dtm_train_tfidf <- fit_transform(dtm_train, tfidf)

  # Tokenize test data and apply training vocab
  it_test <- itoken(test$description, 
                    preprocessor = prep_fun, 
                    tokenizer = tok_fun, 
                    ids = test$X1, 
                    progressbar = T)
  
  dtm_test_tfidf <- create_dtm(it_test, vectorizer) %>% 
    transform(tfidf)
  
  list(
    'train' = as.matrix(cbind(dtm_train_tfidf, train$price)),
    'test' = as.matrix(cbind(dtm_test_tfidf, test$price))
  )
  
}
```

5-fold CV

```{r}
folds <- createFolds(y = train$points, k = 5)

predicted <- data.frame()
all_rmse <- c()

#doesn't work because can't install lightgbm
for(i in 1:length(folds)) {
  
  print(paste('CV fold', i, 'of', length(folds), sep = ' '))
  
  train_set <- train[-folds[[i]],]
  valid_set <- train[folds[[i]],]
  cleaned <- clean(train_set, valid_set)
  train_inputs <- cleaned[['train']]
  valid_inputs <- cleaned[['test']]
  
  train_labels <- as.matrix(train[-folds[[i]], 'points'])
  valid_labels <- as.matrix(train[folds[[i]], 'points'])

  dtrain <- lgb.Dataset(data = train_inputs, label = train_labels)
  dtest <- lgb.Dataset.create.valid(dtrain, data = valid_inputs, label = valid_labels)
  valids <- list(test = dtest)
  
  model <- lgb.train(objective = 'regression', 
                     metric = 'rmse',
                     data = dtrain,
                     num_iterations = 10000,
                     valid = valids,
                     learning_rate = 0.1,
                     num_leaves = 50,
                     early_stopping_rounds = 50,
                     verbosity = -1,
                     verbose = -1,
                     record = TRUE)
  
  all_rmse <- c(all_rmse, model$best_score)
  
  # Predict on the validation set
  predictions <- predict(model, valid_inputs)
  
  predicted <- rbind(predicted, data.frame(
    predicted = predictions,
    rating = valid_labels
  ))
}
```


**K-Prototypes Clustering**

```{r}
library(clustMixType)
# apply k-prototypes
kpres <- kproto(Wine3, 20)
```



