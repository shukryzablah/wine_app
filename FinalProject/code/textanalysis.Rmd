---
title: "Text analysis"
author: "YZ Analytics"
date: ""
output: 
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---

```{r, setup, include=FALSE}
library(tidyverse)
library(tm)
library(text2vec)
library(tidytext)
library(textmineR)
library(SnowballC)
library(caret)
library(gbm)
```

```{r}
Wine <- read_csv("../data/winemag-data-130k-v2.csv",
                 col_types = cols(
                     X1 = col_double(),
                     country = col_character(),
                     description = col_character(),
                     designation = col_character(),
                     points = col_double(),
                     price = col_double(),
                     province = col_character(),
                     region_1 = col_character(),
                     region_2 = col_character(),
                     taster_name = col_character(),
                     taster_twitter_handle = col_character(),
                     title = col_character(),
                     variety = col_character(),
                     winery = col_character()),
                 progress = FALSE
                 ) %>%
    rename(id = X1)
```


### Term Frequency-Inverse Document Frequency

Code based from https://www.tidytextmining.com/tfidf.html.

Term Frequency-Inverse Document Frequency (TF-IDF) is a statistic that evaluates how important a word is in a document or corpus. It is calculated through dividing the term frequency of how often a word appears in a document by its inverse document frequency, which is the inverse of the proportion of how many documents in a corpus have a certain term.  Therefore, high frequency terms but with little importance such as "the" or "and" will have low TF-IDF values, and so TF-IDF can be used as a weighting measure in ranking.

To use TF-IDF in our prediction model, we will use a sum of the TF-IDF per description as a variable.

```{r}
Wine_tfidf <- Wine %>%
  unnest_tokens(word, description) %>%
  count(points, id, word, sort = TRUE) %>%
  bind_tf_idf(word, id, n) %>%
  group_by(id) %>% 
  summarise(tf_idf = sum(tf_idf))

Wine2 <- left_join(Wine, Wine_tfidf, by = "id")
```

First let's look at TF-IDF by points:

```{r}
Wine_points_tfidf <- Wine %>%
  unnest_tokens(word, description) %>%
  count(points, word, sort = TRUE) %>%
  bind_tf_idf(word, points, n)
```

```{r}
Wine_points_tfidf %>%
  filter(points == 100) %>%
  arrange(desc(tf_idf)) %>%
  head()
```

We see that the words with the highest TF-IDF values are the unique words in the 100-point wine descriptions that occur only 1-2 in the vocabulary of all the descriptions. 

Let's look specifically at the words with the highest TF-IDF values for 80-point wines:

```{r}
Wine_points_tfidf %>%
  filter(points == 80) %>%
  arrange(desc(tf_idf)) %>%
  head()
```

These words occur more frequently than the words in the 100-point descriptions. However, the frequencies are pretty low. It might be useful to separate points into different levels (perhaps 80-86 is low rating, 97-93 is medium, and 94-100 is high).

```{r}
Wine$rating <- cut(Wine$points,
                   breaks=c(-Inf, 86, 93, Inf),
                   labels=c("low","medium","high"))
```

```{r}
Wine_rating_tfidf <- Wine %>%
  unnest_tokens(word, description) %>%
  count(rating, word, sort = TRUE) %>%
  bind_tf_idf(word, rating, n)
```

```{r}
Wine_rating_tfidf %>%
  filter(rating == "high") %>%
  arrange(desc(tf_idf)) %>%
  head()
```

Following https://www.kaggle.com/nnnnick/predicting-wine-ratings-using-lightgbm-text2vec, we can look at words with the highest and lowest mean scores:

```{r}
wine_explore <- Wine %>% 
    select(description, points) %>%
    mutate(description = gsub('[[:punct:] ]+',' ',tolower(description)))

words <- str_split(wine_explore$description, ' ')
all_words <- data.frame(points = rep(wine_explore$points, sapply(words, length)), words = unlist(words))
```

```{r}
words_grouped <- all_words %>%
    group_by(words) %>%
    summarize(
        points = mean(points),
        count = n()
    ) %>%
    filter(count > 10) %>%
    arrange(desc(points))

top <- words_grouped[1:10,] %>% cbind(top_bottom = 'top')
bottom <- words_grouped[(nrow(words_grouped) - 9):nrow(words_grouped),] %>% cbind(top_bottom = 'bottom')
top_bottom <- rbind(top, bottom)

ggplot(top_bottom, aes(x = reorder(words, points), y = points, fill = top_bottom)) +
    geom_bar(stat = 'identity') +
    coord_flip() + 
    scale_fill_manual(values = c('#00b4fb', '#fa6560')) + 
    ggtitle('Wine Review Words with the Highest and Lowest Mean Points', subtitle = NULL) +
    xlab('Average Points') +
    ylab('Word') + 
    labs(fill = 'Top or Bottom')
```

Nobody wants to drink a wine that's described as "gluey" or "fake." How often do these words show up though, and how can we use them in a predictive model? To find out, we need to create a document-term matrix, which shows the the frequency of terms that occur in a collection of documents.

### Creating a Document-Term Matrix

DTM created from code here: https://datawarrior.wordpress.com/2018/01/22/document-term-matrix-text-mining-in-r-and-python/

```{r}
#Create DTM
dtm <- CreateDtm(Wine$description,
                doc_names = Wine$id,
                ngram_window = c(1, 1),
                lower = TRUE,
                remove_punctuation = TRUE,
                remove_numbers = TRUE,
                stem_lemma_function = wordStem)
```

The document-term matrix that is created is huge - about 43 megabytes with close to 3 billion elements. We need to create some functions that will allow us to use the DTM:

```{r}
#Create functions
get.token.occurrences<- function(dtm, token)
  dtm[, token] %>% as.data.frame() %>% rename(count=".") %>%
  mutate(token=row.names(.)) %>% arrange(-count)
 
get.total.freq<- function(dtm, token) dtm[, token] %>% sum
 
get.doc.freq<- function(dtm, token)
  dtm[, token] %>% as.data.frame() %>% rename(count=".") %>%
  filter(count>0) %>% pull(count) %>% length
```

Now we can see how many wines are actually described as "fake":

```{r}
dtm %>% get.doc.freq(wordStem("fake"))
```

Which 13 wines?

```{r}
fakewines <- dtm %>% get.token.occurrences(wordStem("fake")) %>% head(13)
Wine$title[c(as.numeric(fakewines$token))]
```

Let's look at the description of the fourth wine, Love 2015 Cabernet Sauvignon (Vino de la Tierra de Castilla):

```{r}
Wine$description[27585] #27585 is the token number for the fourth wine
```

Yikes. This seems like a bad wine.

Let's look at the most frequent terms:

```{r}
tf_mat <- TermDocFreq(dtm = dtm)
head(tf_mat[ order(tf_mat$term_freq, decreasing = TRUE) , ], 10)
```

Unsurprisingly, the most frequently used words in the descriptions are "wine," "flavor," and "fruit."

Now let's see how we can use the DTM in a data frame for prediction.

```{r}
# remove any tokens that were in 2000 or fewer documents
dtm_small <- dtm[ , colSums(dtm > 0) > 2000 ]
ncol(dtm_small)
```

We're left with 279 words that are used in more than 2000 documents in the form of indicator variables per wine.

### Prediction

Let's look at predicting wines.

Because we have a lot of missing data and a mixture of numerical and categorical data, methods like random forest are difficult to implement. Let's try gradient boosting, which in R can include categorical variables of up to 1024 categories (unlike randomForest, which only allows up to 53 categories per categorical variable).

First, we need to clean our data:

```{r}
dtm_df <- as.data.frame(as.matrix(dtm_small))
dtm_df$id <- c(0:129970)
Wine_dtm <- left_join(Wine2, dtm_df, by = "id")
Wine_dtm <- Wine_dtm %>%
  select(-id, -description, -designation, -region_1, -region_2, -taster_name,
         -taster_twitter_handle, -title, -winery) %>%
  mutate(country = as.factor(country),
         province = as.factor(province),
         variety = as.factor(variety))
```

```{r}
#Test/Train split
set.seed(1)
smp_size <- floor(0.8 * nrow(Wine_dtm))
train_ind <- sample(seq_len(nrow(Wine_dtm)), size = smp_size)
train <- Wine_dtm[train_ind, ]
test <- Wine_dtm[-train_ind, ]
```

We'll use a gradient boosting algorithm to create our prediction model.

```{r, cache = TRUE}
#boosting
set.seed(1)
boost_wine <- gbm(points ~ ., 
                   data = train, 
                   distribution = "gaussian", 
                   n.trees = 1000,
                   interaction.depth = 4)
```

We can check to see how well our model does with prediction on the test set:

```{r}
boost_estimate <- predict(boost_wine, 
                         newdata = test, 
                         n.trees = 1000)
mse_boost <- mean((boost_estimate - test$points)^2); mse_boost
```

The mean squared error is `r mse_boost`. Let's look at the most important features in the model.

```{r}
top_n(summary(boost_wine), 20, rel.inf)
```

Unsurprisingly, the variable with the most relative influence is price, followed by variety, then province.  The words "rich" and "complex" have slightly higher influence than the tf_idf variable, then followed by the stemmed words "simpl," "delici," "long," and "black."


