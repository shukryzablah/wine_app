---
title: "Text analysis"
author: "YZ Analytics"
date: ""
output: 
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---

```{r, setup, include=FALSE}
library(tidyverse)
library(tm)
library(text2vec)
library(tidytext)
library(randomForest)
```

```{r}
Wine <- read_csv("../data/winemag-data-130k-v2.csv",
                 col_types = cols(
                     X1 = col_double(),
                     country = col_character(),
                     description = col_character(),
                     designation = col_character(),
                     points = col_double(),
                     price = col_double(),
                     province = col_character(),
                     region_1 = col_character(),
                     region_2 = col_character(),
                     taster_name = col_character(),
                     taster_twitter_handle = col_character(),
                     title = col_character(),
                     variety = col_character(),
                     winery = col_character()),
                 progress = FALSE
                 ) %>%
    rename(id = X1)
```


### Calculating Term Frequency-Inverse Document Frequency

Code based from https://www.tidytextmining.com/tfidf.html.

Term Frequency-Inverse Document Frequency (TF-IDF) is a statistic that evaluates how important a word is in a document or corpus. It is calculated through dividing the term frequency of how often a word appears in a document by its inverse document frequency, which is the inverse of the proportion of how many documents in a corpus have a certain term.  Therefore, high frequency terms but with little importance such as "the" or "and" will have low TF-IDF values, and so TF-IDF can be used as a weighting measure in ranking.

To use TF-IDF in our prediction model, we will use a sum of the TF-IDF per description as a variable.

```{r}
Wine_tfidf <- Wine %>%
  unnest_tokens(word, description) %>%
  count(points, id, word, sort = TRUE) %>%
  bind_tf_idf(word, id, n) %>%
  group_by(id) %>% 
  summarise(tf_idf = sum(tf_idf))

Wine <- left_join(Wine, Wine_tfidf, by = "id")
```

First let's look at TF-IDF by points:

```{r}
Wine_points_tfidf <- Wine %>%
  unnest_tokens(word, description) %>%
  count(points, word, sort = TRUE) %>%
  bind_tf_idf(word, points, n)
```

```{r}
Wine_points_tfidf %>%
  filter(points == 100) %>%
  arrange(desc(tf_idf)) %>%
  head()
```

We see that the words with the highest TF-IDF values are the unique words in the 100-point wine descriptions that occur only 1-2 in the vocabulary of all the descriptions. 

Let's look specifically at the words with the highest TF-IDF values for 80-point wines:

```{r}
Wine_points_tfidf %>%
  filter(points == 80) %>%
  arrange(desc(tf_idf)) %>%
  head()
```

These words occur more frequently than the words in the 100-point descriptions. However, the frequencies are pretty low. It might be useful to separate points into different levels (perhaps 80-86 is low rating, 97-93 is medium, and 94-100 is high).

```{r}
Wine$rating <- cut(Wine$points,
                   breaks=c(-Inf, 86, 93, Inf),
                   labels=c("low","medium","high"))
```

```{r}
Wine_rating_tfidf <- Wine %>%
  unnest_tokens(word, description) %>%
  count(rating, word, sort = TRUE) %>%
  bind_tf_idf(word, rating, n)
```

```{r}
Wine_rating_tfidf %>%
  filter(rating == "high") %>%
  arrange(desc(tf_idf)) %>%
  head()
```

We can also look at TF-IDF based on variables other than points. For example, we can look at TF-IDF values based on variety of wine.

```{r}
wine_words <- Wine %>%
  unnest_tokens(word, description) %>%
  count(variety, word, sort = TRUE)
```

```{r}
plot_wine <- wine_words %>%
  bind_tf_idf(word, variety, n) %>%
  mutate(word = fct_reorder(word, tf_idf)) %>%
  mutate(variety = factor(variety, levels = c("Pinot Noir",
                                            "Cabernet Sauvignon", 
                                            "Chardonnay",
                                            "Red Blend", 
                                            "Riesling")))

plot_wine %>% 
  group_by(variety) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = variety)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~variety, ncol = 2, scales = "free") +
  coord_flip()
```

A lot of the words with high TF-IDF values by variety are obvious - for example, the word with the highest TF-IDF value for riesling is "riesling."

Following https://www.kaggle.com/nnnnick/predicting-wine-ratings-using-lightgbm-text2vec, we can look at words with the highest and lowest mean scores:

```{r}
wine_explore <- Wine %>% 
    select(description, points) %>%
    mutate(description = gsub('[[:punct:] ]+',' ',tolower(description)))

words <- str_split(wine_explore$description, ' ')
all_words <- data.frame(points = rep(wine_explore$points, sapply(words, length)), words = unlist(words))
```

```{r}
words_grouped <- all_words %>%
    group_by(words) %>%
    summarize(
        points = mean(points),
        count = n()
    ) %>%
    filter(count > 10) %>%
    arrange(desc(points))

top <- words_grouped[1:10,] %>% cbind(top_bottom = 'top')
bottom <- words_grouped[(nrow(words_grouped) - 9):nrow(words_grouped),] %>% cbind(top_bottom = 'bottom')
top_bottom <- rbind(top, bottom)

ggplot(top_bottom, aes(x = reorder(words, points), y = points, fill = top_bottom)) +
    geom_bar(stat = 'identity') +
    coord_flip() + 
    scale_fill_manual(values = c('#00b4fb', '#fa6560')) + 
    ggtitle('Wine Review Words with the Highest and Lowest Mean Points', subtitle = NULL) +
    xlab('Average Points') +
    ylab('Word') + 
    labs(fill = 'Top or Bottom')
```

### Prediction

Let's look at predicting wines.

Because we have a lot of missing data and a mixture of numerical and categorical data, methods like random forest are difficult to implement.

could look at k-prototypes clustering? https://cran.r-project.org/web/packages/clustMixType/clustMixType.pdf or k-modes clustering?
could also look at gradient boosting like in the example above

**Light gradient boosting**

First, we'll adjust our dataset and remove the id, description, title, and taster_twitter_handle variables.

```{r}
Wine2 <- Wine %>%
  select(-id, -description, -title, -taster_twitter_handle)
```


Next, we will split our data into training and test sets.

```{r}
set.seed(1)
x <- model.matrix(points ~ ., Wine2)[ , -1]  
y <- Wine2$points
grid <- 10^seq(10, -2, length = 100)
n <- nrow(Wine2)
train_index <- sample(1:n, 0.8 * n)
test_index <- setdiff(1:n, train_index)
train <- Wine2[train_index, ] 
test <- Wine2[test_index, ]
y.test <- y[test_index]
```


